/**
 * useEmotionDetection Hook
 * ìŒì„± í†¤ ê¸°ë°˜ ê°ì • ê°ì§€ ì‹œìŠ¤í…œ í´ë¼ì´ì–¸íŠ¸ í›…
 *
 * Web Audio APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ë° ê°ì • ë¶„ì„
 */

import { useState, useCallback, useRef, useEffect } from "react";
import { getFunctions, httpsCallable } from "firebase/functions";

export type EmotionType =
  | "stressed"
  | "excited"
  | "tired"
  | "positive"
  | "negative"
  | "neutral";

export interface VoiceFeatures {
  pitch: number;
  pitchVariation: number;
  energy: number;
  tempo: number;
  pauseFrequency: number;
}

export interface EmotionResult {
  primary: EmotionType;
  confidence: number;
  intensity: number;
  features: VoiceFeatures;
}

export interface EmotionStyle {
  tone: string;
  encouragement?: string;
  difficultyAdjustment: number;
  responseLength: "short" | "medium" | "long";
  includeHumor: boolean;
  includeEmoji: boolean;
}

export interface EmotionDetectionState {
  currentEmotion: EmotionType;
  emotionConfidence: number;
  emotionIntensity: number;
  style: EmotionStyle | null;
  suggestions: string[];
  isAnalyzing: boolean;
  error: string | null;
  emotionHistory: Array<{ emotion: EmotionType; timestamp: Date }>;
}

export interface UseEmotionDetectionReturn {
  state: EmotionDetectionState;
  analyzeEmotion: (
    voiceFeatures?: VoiceFeatures,
    text?: string
  ) => Promise<void>;
  extractVoiceFeatures: (audioBlob: Blob) => Promise<VoiceFeatures>;
  resetState: () => void;
  getEmotionLabel: (emotion: EmotionType) => string;
  getEmotionEmoji: (emotion: EmotionType) => string;
}

/**
 * ê°ì • ê°ì§€ Hook
 */
export function useEmotionDetection(
  userId: string | null,
  sessionId: string | null
): UseEmotionDetectionReturn {
  const [state, setState] = useState<EmotionDetectionState>({
    currentEmotion: "neutral",
    emotionConfidence: 0,
    emotionIntensity: 0,
    style: null,
    suggestions: [],
    isAnalyzing: false,
    error: null,
    emotionHistory: [],
  });

  const audioContextRef = useRef<AudioContext | null>(null);

  // Audio Context ì´ˆê¸°í™”
  useEffect(() => {
    if (typeof window !== "undefined" && !audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
    }

    return () => {
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  /**
   * ê°ì • ë¶„ì„ ì‹¤í–‰
   */
  const analyzeEmotion = useCallback(
    async (voiceFeatures?: VoiceFeatures, text?: string) => {
      if (!userId || !sessionId) {
        console.warn("User ID or Session ID is missing");
        return;
      }

      if (!voiceFeatures && !text) {
        console.warn("Either voice features or text is required");
        return;
      }

      setState((prev) => ({ ...prev, isAnalyzing: true, error: null }));

      try {
        const functions = getFunctions();
        const analyzeEmotionFn = httpsCallable(functions, "analyzeEmotion");

        const result = await analyzeEmotionFn({
          userId,
          sessionId,
          voiceFeatures,
          text,
        });

        const response = result.data as any;

        setState((prev) => ({
          ...prev,
          currentEmotion: response.emotion.primary,
          emotionConfidence: response.emotion.confidence,
          emotionIntensity: response.emotion.intensity,
          style: response.style,
          suggestions: response.suggestions,
          emotionHistory: [
            ...prev.emotionHistory,
            {
              emotion: response.emotion.primary,
              timestamp: new Date(),
            },
          ],
          isAnalyzing: false,
        }));

        // ê°ì • ì „í™˜ ê°ì§€
        if (
          state.emotionHistory.length > 0 &&
          state.currentEmotion !== response.emotion.primary
        ) {
          console.log(
            `ê°ì • ì „í™˜: ${state.currentEmotion} â†’ ${response.emotion.primary}`
          );
        }
      } catch (error: any) {
        console.error("Emotion analysis error:", error);
        setState((prev) => ({
          ...prev,
          error: error.message || "ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
          isAnalyzing: false,
        }));
      }
    },
    [userId, sessionId, state.currentEmotion, state.emotionHistory]
  );

  /**
   * ìŒì„± íŠ¹ì§• ì¶”ì¶œ (Web Audio API)
   */
  const extractVoiceFeatures = useCallback(
    async (audioBlob: Blob): Promise<VoiceFeatures> => {
      if (!audioContextRef.current) {
        throw new Error("AudioContext not initialized");
      }

      const audioContext = audioContextRef.current;

      // Blobì„ ArrayBufferë¡œ ë³€í™˜
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

      // ì±„ë„ ë°ì´í„° ì¶”ì¶œ
      const channelData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;

      // 1. Pitch ì¶”ì •
      const pitch = estimatePitch(channelData, sampleRate);

      // 2. Pitch Variation (ê°„ë‹¨ ë²„ì „)
      const pitchVariation = 35; // ê¸°ë³¸ê°’ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚° í•„ìš”)

      // 3. Energy (RMS)
      const energy = calculateRMS(channelData);

      // 4. Tempo (ì„ì‹œ, ìŒì„± ì¸ì‹ ê²°ê³¼ í•„ìš”)
      const tempo = 120; // ê¸°ë³¸ê°’

      // 5. Pause Frequency
      const pauseFrequency = detectPauseFrequency(channelData);

      return {
        pitch,
        pitchVariation,
        energy,
        tempo,
        pauseFrequency,
      };
    },
    []
  );

  /**
   * ìƒíƒœ ì´ˆê¸°í™”
   */
  const resetState = useCallback(() => {
    setState({
      currentEmotion: "neutral",
      emotionConfidence: 0,
      emotionIntensity: 0,
      style: null,
      suggestions: [],
      isAnalyzing: false,
      error: null,
      emotionHistory: [],
    });
  }, []);

  return {
    state,
    analyzeEmotion,
    extractVoiceFeatures,
    resetState,
    getEmotionLabel,
    getEmotionEmoji,
  };
}

/**
 * Pitch ì¶”ì • (ìê¸°ìƒê´€ í•¨ìˆ˜)
 */
function estimatePitch(audioData: Float32Array, sampleRate: number): number {
  const minFreq = 80; // 80 Hz
  const maxFreq = 400; // 400 Hz
  const minPeriod = Math.floor(sampleRate / maxFreq);
  const maxPeriod = Math.floor(sampleRate / minFreq);

  let bestPeriod = 0;
  let bestCorrelation = 0;

  for (let period = minPeriod; period <= maxPeriod; period++) {
    let correlation = 0;
    for (let i = 0; i < audioData.length - period; i++) {
      correlation += audioData[i] * audioData[i + period];
    }

    if (correlation > bestCorrelation) {
      bestCorrelation = correlation;
      bestPeriod = period;
    }
  }

  return bestPeriod > 0 ? sampleRate / bestPeriod : 200;
}

/**
 * RMS (Root Mean Square) ê³„ì‚°
 */
function calculateRMS(audioData: Float32Array): number {
  let sum = 0;
  for (let i = 0; i < audioData.length; i++) {
    sum += audioData[i] * audioData[i];
  }
  const rms = Math.sqrt(sum / audioData.length);
  return Math.min(1, rms * 10); // 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
}

/**
 * ë¬´ìŒ êµ¬ê°„ ë¹ˆë„ ê°ì§€
 */
function detectPauseFrequency(audioData: Float32Array): number {
  const threshold = 0.01;
  let silentSamples = 0;

  for (let i = 0; i < audioData.length; i++) {
    if (Math.abs(audioData[i]) < threshold) {
      silentSamples++;
    }
  }

  return silentSamples / audioData.length;
}

/**
 * ê°ì • ë¼ë²¨ ë°˜í™˜
 */
function getEmotionLabel(emotion: EmotionType): string {
  const labels: Record<EmotionType, string> = {
    stressed: "ìŠ¤íŠ¸ë ˆìŠ¤/ë¶ˆì•ˆ",
    excited: "í¥ë¶„/ì—´ì •",
    tired: "í”¼ê³¤/ì§€ì¹¨",
    positive: "ê¸ì •/ì¦ê±°ì›€",
    negative: "ë¶€ì •/ì¢Œì ˆ",
    neutral: "ì¤‘ë¦½/í‰ì˜¨",
  };
  return labels[emotion];
}

/**
 * ê°ì • ì´ëª¨ì§€ ë°˜í™˜
 */
function getEmotionEmoji(emotion: EmotionType): string {
  const emojis: Record<EmotionType, string> = {
    stressed: "ğŸ˜°",
    excited: "ğŸ¤©",
    tired: "ğŸ˜´",
    positive: "ğŸ˜Š",
    negative: "ğŸ˜”",
    neutral: "ğŸ˜",
  };
  return emojis[emotion];
}
